


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

%matplotlib inline


# Datasets:

df_final_web_data_pt_1 = pd.read_csv('../data/raw/df_final_web_data_pt_1.txt', sep=',')
df_final_web_data_pt_2 = pd.read_csv('../data/raw/df_final_web_data_pt_2.txt', sep=',')

df_final_experiment_clients = pd.read_csv('../data/raw/df_final_experiment_clients.txt', sep=',')


# Let's start with the exploration and cleaning of 'df_final_web_data_pt_1'
df_final_web_data_pt_1.shape


# Checking for duplicates in 'df_final_web_data_pt_1'
df_final_web_data_pt_1.duplicated().sum()


# Remove duplicates
df_final_web_data_pt_1 = df_final_web_data_pt_1.drop_duplicates()

# Verify if duplicates are removed
df_final_web_data_pt_1.shape


# Filtering out columns where more than 80% of the values are missing
df_final_web_data_pt_1 = df_final_web_data_pt_1[df_final_web_data_pt_1.columns[df_final_web_data_pt_1.isnull().mean() < 0.8]]
df_final_web_data_pt_1.shape


# Checking for null values in 'df_final_web_data_pt_2'
df_final_web_data_pt_1.isnull().sum()


df_final_web_data_pt_1.head()


# Now, let's continue with the exploration and cleaning of 'df_final_web_data_pt_2'
df_final_web_data_pt_2.shape


# Checking for duplicates in 'df_final_web_data_pt_2'
df_final_web_data_pt_2.duplicated().sum()


# Remove duplicates
df_final_web_data_pt_2 = df_final_web_data_pt_2.drop_duplicates()

# Verify if duplicates are removed
df_final_web_data_pt_2.shape


# Filtering out columns where more than 80% of the values are missing
df_final_web_data_pt_2 = df_final_web_data_pt_2[df_final_web_data_pt_2.columns[df_final_web_data_pt_2.isnull().mean() < 0.8]]
df_final_web_data_pt_2.shape


# Checking for null values in 'df_final_web_data_pt_2'
df_final_web_data_pt_2.isnull().sum()


df_final_web_data_pt_2.head()


# Merge Digital Footprints datasets: 'df_final_web_data_pt_1' and 'df_final_web_data_pt_2'
df_final_web_data = pd.concat([df_final_web_data_pt_1, df_final_web_data_pt_2], ignore_index=True)
df_final_web_data.shape


# Checking for null values in df_final_web_data.
df_final_web_data.isnull().sum()


df_final_web_data.head()


df_final_web_data.shape


# Now we will merge the dataset 'df_final_web_data' with 'df_final_experiment_clients' in order
# to have in just one dataset all the data related to the AB test

# First, let's explore and clean the 'df_final_experiment_clients'

df_final_experiment_clients.head()


df_final_experiment_clients.shape


# Checking for duplicates in 'df_final_experiment_clients'
df_final_experiment_clients.duplicated().sum()


# Filtering out columns where more than 80% of the values are missing
df_final_experiment_clients = df_final_experiment_clients[df_final_experiment_clients.columns[df_final_experiment_clients.isnull().mean() < 0.8]]
df_final_experiment_clients.shape


# Checking for null values in 'df_final_experiment_clients'
df_final_experiment_clients.isnull().sum()


# Drop rows with null values in 'df_final_experiment_clients'
df_final_experiment_clients = df_final_experiment_clients.dropna()

# Verify if nulls are removed
df_final_experiment_clients.isnull().sum()


df_final_experiment_clients.shape


# We will analyze only the observations that are part of the AB test, specifically those with a defined 'Variation'.
# To achieve this, we will perform an inner join between 'df_final_experiment_clients' and 'df_final_web_data'

# Perform an inner join on the two DataFrames using the 'client_id' column
df_final_experiment_web_data = pd.merge(df_final_web_data, df_final_experiment_clients, on='client_id', how='inner')

df_final_experiment_web_data.head()


df_final_experiment_web_data.shape





df_final_demo = pd.read_csv("../data/raw/df_final_demo.txt", delimiter=",")
df_final_demo.shape


df_final_demo.head()


# Checking for duplicates in 'df_final_experiment_clients'
df_final_demo.duplicated().sum()


df_final_demo.isnull().sum()


# Filtering out columns where more than 80% of the values are missing
df_final_demo = df_final_demo[df_final_demo.columns[df_final_demo.isnull().mean() < 0.8]]
df_final_demo.shape


df_final_demo.isnull().sum()


# Perform an inner join on the two DataFrames using the 'client_id' column
# 'df_final_experiment_web_data' and 'df_final_demo'
df_final = pd.merge(df_final_experiment_web_data, df_final_demo, on='client_id', how='inner')

df_final.head()


df_final.shape


# export the final dataframe to a CSV file
df_final.to_csv('df_final', index=False)


# Now we will explore the final dataframe 'df_final'

# Analysing the unique values
df_final.nunique()


df_final['process_step'].unique()





import plotly.graph_objects as go

# Data
stages = ['start', 'step_1', 'step_2', 'step_3', 'confirm']
control = [45380, 29544, 25773, 22503, 17336] # Funnel A
test = [55773, 38666, 30899, 22503, 17336] # Funnel B


fig = go.Figure()

# Funnel A
fig.add_trace(go.Funnel(
    name='Control',
    y=stages,
    x=control,
    textinfo="value+percent initial"))

# Funnel B
fig.add_trace(go.Funnel(
    name='Test',
    y=stages,
    x=test,
    textinfo="value+percent initial"))

fig.update_layout(title="A/B Test Funnel Comparison")

fig.show()


def calculate_dropoff_percentages(funnel):
    dropoffs = []
    for i in range(1, len(funnel)):
        dropoff = (funnel[i-1] - funnel[i]) / funnel[i-1] * 100
        dropoffs.append(dropoff)
    return dropoffs

# Calculate drop-off percentages
control_dropoffs = calculate_dropoff_percentages(control)
test_dropoffs = calculate_dropoff_percentages(test)

# Display the drop-off percentages
control_dropoffs, test_dropoffs


import plotly.graph_objects as go

# Function to calculate drop-off percentages
def calculate_dropoff_percentages(funnel):
    dropoffs = []
    for i in range(1, len(funnel)):
        dropoff = (funnel[i-1] - funnel[i]) / funnel[i-1] * 100
        dropoffs.append(f"{dropoff:.2f}%")
    dropoffs.insert(0, "Start")  # No drop-off for the first step
    return dropoffs

# Data
stages = ['start', 'step_1', 'step_2', 'step_3', 'confirm']
control = [45380, 29544, 25773, 22503, 17336]  # Funnel A
test = [55773, 38666, 30899, 22503, 17336]    # Funnel B

# Calculate drop-off percentages
control_dropoffs = calculate_dropoff_percentages(control)
test_dropoffs = calculate_dropoff_percentages(test)

# Create the funnel chart with larger figure size
fig = go.Figure()

# Funnel A with larger drop-off percentages inside the funnel
fig.add_trace(go.Funnel(
    name='Control',
    y=stages,
    x=control,
    text=control_dropoffs,
    textinfo="text+value+percent initial",
    textposition="inside",
    textfont=dict(size=18)  # Increase font size specifically for inside funnel
))

# Funnel B with larger drop-off percentages inside the funnel
fig.add_trace(go.Funnel(
    name='Test',
    y=stages,
    x=test,
    text=test_dropoffs,
    textinfo="text+value+percent initial",
    textposition="inside",
    textfont=dict(size=18)  # Increase font size specifically for inside funnel
))

# Increase the size of the figure to allow more space for text
fig.update_layout(
    title="A/B Test Funnel Comparison with Larger Inside Drop-off Percentages",
    height=600,  # Increase height to make more room
    width=800   # Increase width if necessary
)

fig.show()




